#!/usr/bin/env python3
"""
Collaborative Research Agents - AIå®Ÿé¨“å®Ÿè£…ä¾‹
azukiazusa1ã‚¹ã‚¿ã‚¤ãƒ«ã®å®Ÿç”¨çš„ãƒ‡ãƒ¢ã‚³ãƒ¼ãƒ‰

è¤‡æ•°ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦æŠ€è¡“ç ”ç©¶ã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ 
- Research Agent: è«–æ–‡ãƒ»è¨˜äº‹èª¿æŸ»
- Analysis Agent: æŠ€è¡“åˆ†æãƒ»è©•ä¾¡
- Synthesis Agent: çŸ¥è­˜çµ±åˆãƒ»æ´å¯Ÿç”Ÿæˆ
- Writer Agent: è¨˜äº‹ä½œæˆãƒ»å“è³ªãƒã‚§ãƒƒã‚¯
"""

import asyncio
import json
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import aiohttp
import openai
from enum import Enum

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AgentRole(Enum):
    RESEARCHER = "researcher"
    ANALYST = "analyst"
    SYNTHESIZER = "synthesizer"
    WRITER = "writer"


@dataclass
class ResearchTask:
    """ç ”ç©¶ã‚¿ã‚¹ã‚¯å®šç¾©"""
    id: str
    topic: str
    research_questions: List[str]
    target_domains: List[str]
    deadline: str
    priority: int
    status: str = "pending"


@dataclass
class ResearchResult:
    """ç ”ç©¶çµæœ"""
    agent_id: str
    task_id: str
    result_type: str  # "literature", "analysis", "synthesis", "article"
    content: Dict[str, Any]
    confidence_score: float
    created_at: str


class BaseAgent:
    """åŸºåº•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, agent_id: str, role: AgentRole, api_key: str):
        self.agent_id = agent_id
        self.role = role
        self.api_key = api_key
        self.openai_client = openai.AsyncOpenAI(api_key=api_key)
        self.results_history: List[ResearchResult] = []
    
    async def process_task(self, task: ResearchTask) -> ResearchResult:
        """ã‚¿ã‚¹ã‚¯å‡¦ç† - ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…"""
        raise NotImplementedError
    
    async def _call_llm(self, system_prompt: str, user_prompt: str, 
                       model: str = "gpt-4") -> str:
        """LLM APIå‘¼ã³å‡ºã—"""
        try:
            response = await self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"LLM API error: {e}")
            return ""
    
    def save_result(self, result: ResearchResult):
        """çµæœä¿å­˜"""
        self.results_history.append(result)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚ä¿å­˜
        results_dir = Path(f"results/{self.agent_id}")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = results_dir / f"{result.task_id}_{result.result_type}.json"
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)


class ResearchAgent(BaseAgent):
    """æ–‡çŒ®èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, agent_id: str, api_key: str):
        super().__init__(agent_id, AgentRole.RESEARCHER, api_key)
    
    async def process_task(self, task: ResearchTask) -> ResearchResult:
        """æ–‡çŒ®èª¿æŸ»å®Ÿè¡Œ"""
        logger.info(f"[{self.agent_id}] Starting literature research: {task.topic}")
        
        # 1. æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
        keywords = await self._generate_search_keywords(task)
        
        # 2. è«–æ–‡æ¤œç´¢ (arXiv APIä½¿ç”¨)
        papers = await self._search_arxiv_papers(keywords, limit=10)
        
        # 3. GitHub ãƒªãƒã‚¸ãƒˆãƒªæ¤œç´¢
        repos = await self._search_github_repos(keywords, limit=5)
        
        # 4. æŠ€è¡“è¨˜äº‹æ¤œç´¢
        articles = await self._search_tech_articles(keywords, limit=10)
        
        # 5. çµæœçµ±åˆ
        literature_review = {
            "keywords": keywords,
            "papers": papers,
            "repositories": repos,
            "articles": articles,
            "summary": await self._generate_literature_summary(papers, repos, articles)
        }
        
        result = ResearchResult(
            agent_id=self.agent_id,
            task_id=task.id,
            result_type="literature",
            content=literature_review,
            confidence_score=0.8,
            created_at=datetime.now().isoformat()
        )
        
        self.save_result(result)
        return result
    
    async def _generate_search_keywords(self, task: ResearchTask) -> List[str]:
        """æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ"""
        system_prompt = """
        ã‚ãªãŸã¯æŠ€è¡“ç ”ç©¶ã®ãŸã‚ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
        ä¸ãˆã‚‰ã‚ŒãŸç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰ã€åŠ¹æœçš„ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        """
        
        user_prompt = f"""
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {task.topic}
        å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: {', '.join(task.target_domains)}
        ç ”ç©¶è³ªå•: {', '.join(task.research_questions)}
        
        ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã®åŠ¹æœçš„ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’10å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸¡æ–¹ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
        """
        
        response = await self._call_llm(system_prompt, user_prompt)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º (ç°¡æ˜“å®Ÿè£…)
        keywords = [kw.strip() for kw in response.split('\n') if kw.strip()]
        return keywords[:10]
    
    async def _search_arxiv_papers(self, keywords: List[str], limit: int = 10) -> List[Dict]:
        """arXivè«–æ–‡æ¤œç´¢"""
        papers = []
        
        # å®Ÿéš›ã®arXiv APIå‘¼ã³å‡ºã— (ç°¡æ˜“å®Ÿè£…)
        for keyword in keywords[:3]:  # æœ€åˆã®3ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
            try:
                async with aiohttp.ClientSession() as session:
                    url = f"http://export.arxiv.org/api/query?search_query=all:{keyword}&start=0&max_results={limit//3}"
                    async with session.get(url) as response:
                        if response.status == 200:
                            content = await response.text()
                            # XMLè§£æã¯ç°¡ç•¥åŒ–
                            papers.append({
                                "title": f"Sample paper for {keyword}",
                                "authors": ["Author 1", "Author 2"],
                                "abstract": f"Abstract for {keyword} research...",
                                "url": f"https://arxiv.org/abs/sample-{keyword}",
                                "published": "2024-01-01"
                            })
            except Exception as e:
                logger.error(f"arXiv search error: {e}")
        
        return papers
    
    async def _search_github_repos(self, keywords: List[str], limit: int = 5) -> List[Dict]:
        """GitHub ãƒªãƒã‚¸ãƒˆãƒªæ¤œç´¢"""
        repos = []
        
        # GitHub APIæ¤œç´¢ (ç°¡æ˜“å®Ÿè£…)
        for keyword in keywords[:2]:
            repos.append({
                "name": f"awesome-{keyword}",
                "description": f"Awesome {keyword} resources and implementations",
                "url": f"https://github.com/example/awesome-{keyword}",
                "stars": 1234,
                "language": "Python",
                "topics": [keyword, "ai", "machine-learning"]
            })
        
        return repos
    
    async def _search_tech_articles(self, keywords: List[str], limit: int = 10) -> List[Dict]:
        """æŠ€è¡“è¨˜äº‹æ¤œç´¢"""
        articles = []
        
        # æŠ€è¡“è¨˜äº‹æ¤œç´¢ (Zenn, Qiitaç­‰ã®APIä½¿ç”¨ - ç°¡æ˜“å®Ÿè£…)
        for keyword in keywords[:5]:
            articles.append({
                "title": f"{keyword}ã‚’ä½¿ã£ãŸå®Ÿè£…æ–¹æ³•",
                "author": "æŠ€è¡“è€…A",
                "url": f"https://zenn.dev/example/{keyword}-implementation",
                "published": "2024-01-15",
                "tags": [keyword, "tutorial", "implementation"]
            })
        
        return articles
    
    async def _generate_literature_summary(self, papers: List[Dict], 
                                         repos: List[Dict], articles: List[Dict]) -> str:
        """æ–‡çŒ®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        system_prompt = """
        ã‚ãªãŸã¯æŠ€è¡“æ–‡çŒ®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
        åé›†ã•ã‚ŒãŸè«–æ–‡ã€ãƒªãƒã‚¸ãƒˆãƒªã€è¨˜äº‹ã‹ã‚‰åŒ…æ‹¬çš„ãªã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        """
        
        user_prompt = f"""
        åé›†ã•ã‚ŒãŸæ–‡çŒ®:
        
        è«–æ–‡: {len(papers)}æœ¬
        {json.dumps(papers[:3], ensure_ascii=False, indent=2)}
        
        ãƒªãƒã‚¸ãƒˆãƒª: {len(repos)}å€‹
        {json.dumps(repos, ensure_ascii=False, indent=2)}
        
        è¨˜äº‹: {len(articles)}æœ¬
        {json.dumps(articles[:3], ensure_ascii=False, indent=2)}
        
        ã“ã‚Œã‚‰ã®æ–‡çŒ®ã‹ã‚‰ä»¥ä¸‹ã‚’å«ã‚€åŒ…æ‹¬çš„ãªã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„:
        1. ä¸»è¦ãªãƒˆãƒ¬ãƒ³ãƒ‰ã¨ç™ºå±•
        2. é‡è¦ãªæŠ€è¡“çš„èª²é¡Œ
        3. å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¯”è¼ƒ
        4. ä»Šå¾Œã®ç ”ç©¶æ–¹å‘
        """
        
        return await self._call_llm(system_prompt, user_prompt)


class AnalysisAgent(BaseAgent):
    """åˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, agent_id: str, api_key: str):
        super().__init__(agent_id, AgentRole.ANALYST, api_key)
    
    async def process_task(self, task: ResearchTask, 
                          literature_result: ResearchResult) -> ResearchResult:
        """æŠ€è¡“åˆ†æå®Ÿè¡Œ"""
        logger.info(f"[{self.agent_id}] Starting technical analysis: {task.topic}")
        
        literature = literature_result.content
        
        # 1. æŠ€è¡“çš„èª²é¡Œåˆ†æ
        challenges = await self._analyze_technical_challenges(literature)
        
        # 2. å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns = await self._analyze_implementation_patterns(literature)
        
        # 3. æ€§èƒ½è©•ä¾¡ãƒ»æ¯”è¼ƒ
        performance = await self._analyze_performance_metrics(literature)
        
        # 4. æŠ€è¡“æˆç†Ÿåº¦è©•ä¾¡
        maturity = await self._assess_technology_maturity(literature)
        
        analysis_result = {
            "technical_challenges": challenges,
            "implementation_patterns": patterns,
            "performance_analysis": performance,
            "technology_maturity": maturity,
            "recommendations": await self._generate_recommendations(challenges, patterns)
        }
        
        result = ResearchResult(
            agent_id=self.agent_id,
            task_id=task.id,
            result_type="analysis",
            content=analysis_result,
            confidence_score=0.85,
            created_at=datetime.now().isoformat()
        )
        
        self.save_result(result)
        return result
    
    async def _analyze_technical_challenges(self, literature: Dict) -> List[Dict]:
        """æŠ€è¡“çš„èª²é¡Œåˆ†æ"""
        system_prompt = """
        æŠ€è¡“æ–‡çŒ®ã‹ã‚‰ä¸»è¦ãªæŠ€è¡“çš„èª²é¡Œã‚’ç‰¹å®šã—ã€ãã®é‡è¦åº¦ã¨è§£æ±ºã®å›°é›£ã•ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        """
        
        user_prompt = f"""
        æ–‡çŒ®æƒ…å ±:
        {json.dumps(literature['summary'], ensure_ascii=False)}
        
        ã“ã®åˆ†é‡ã®ä¸»è¦ãªæŠ€è¡“çš„èª²é¡Œã‚’ç‰¹å®šã—ã€ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
        - èª²é¡Œå
        - é‡è¦åº¦ (1-10)
        - å›°é›£åº¦ (1-10)  
        - ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        - åˆ¶é™äº‹é …
        """
        
        response = await self._call_llm(system_prompt, user_prompt)
        
        # ç°¡æ˜“è§£æ (å®Ÿéš›ã¯ã‚ˆã‚Šç²¾å¯†ãªæ§‹é€ åŒ–ãŒå¿…è¦)
        return [{"challenge": "Sample Challenge", "importance": 8, "difficulty": 7}]
    
    async def _analyze_implementation_patterns(self, literature: Dict) -> List[Dict]:
        """å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        # å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æãƒ­ã‚¸ãƒƒã‚¯
        return [{"pattern": "Sample Pattern", "frequency": 5, "effectiveness": 8}]
    
    async def _analyze_performance_metrics(self, literature: Dict) -> Dict:
        """æ€§èƒ½åˆ†æ"""
        return {"average_accuracy": 0.85, "training_time": "2 hours", "inference_speed": "100ms"}
    
    async def _assess_technology_maturity(self, literature: Dict) -> Dict:
        """æŠ€è¡“æˆç†Ÿåº¦è©•ä¾¡"""
        return {"maturity_level": "emerging", "adoption_rate": "low", "stability": "experimental"}
    
    async def _generate_recommendations(self, challenges: List[Dict], 
                                      patterns: List[Dict]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        return ["Recommendation 1", "Recommendation 2", "Recommendation 3"]


class SynthesisAgent(BaseAgent):
    """çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, agent_id: str, api_key: str):
        super().__init__(agent_id, AgentRole.SYNTHESIZER, api_key)
    
    async def process_task(self, task: ResearchTask, 
                          literature_result: ResearchResult,
                          analysis_result: ResearchResult) -> ResearchResult:
        """çŸ¥è­˜çµ±åˆå®Ÿè¡Œ"""
        logger.info(f"[{self.agent_id}] Starting knowledge synthesis: {task.topic}")
        
        # 1. ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ´å¯Ÿç™ºè¦‹
        cross_insights = await self._discover_cross_domain_insights(
            literature_result.content, analysis_result.content)
        
        # 2. æ–°è¦ç ”ç©¶æ–¹å‘ææ¡ˆ
        research_directions = await self._propose_research_directions(
            analysis_result.content, task.research_questions)
        
        # 3. å®Ÿè£…ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆ
        implementation_ideas = await self._generate_implementation_ideas(
            literature_result.content, analysis_result.content)
        
        # 4. çµ±åˆã‚µãƒãƒªãƒ¼ä½œæˆ
        synthesis_summary = await self._create_synthesis_summary(
            cross_insights, research_directions, implementation_ideas)
        
        synthesis_result = {
            "cross_domain_insights": cross_insights,
            "research_directions": research_directions,
            "implementation_ideas": implementation_ideas,
            "synthesis_summary": synthesis_summary,
            "novelty_score": await self._calculate_novelty_score(cross_insights)
        }
        
        result = ResearchResult(
            agent_id=self.agent_id,
            task_id=task.id,
            result_type="synthesis",
            content=synthesis_result,
            confidence_score=0.75,
            created_at=datetime.now().isoformat()
        )
        
        self.save_result(result)
        return result
    
    async def _discover_cross_domain_insights(self, literature: Dict, analysis: Dict) -> List[Dict]:
        """ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ´å¯Ÿç™ºè¦‹"""
        system_prompt = """
        ã‚ãªãŸã¯ç•°åˆ†é‡çŸ¥è­˜çµ±åˆã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
        æ–‡çŒ®èª¿æŸ»ã¨æŠ€è¡“åˆ†æã®çµæœã‹ã‚‰ã€åˆ†é‡æ¨ªæ–­çš„ãªæ–°ã—ã„æ´å¯Ÿã‚’ç™ºè¦‹ã—ã¦ãã ã•ã„ã€‚
        """
        
        user_prompt = f"""
        æ–‡çŒ®æƒ…å ±: {json.dumps(literature, ensure_ascii=False)[:1000]}
        åˆ†æçµæœ: {json.dumps(analysis, ensure_ascii=False)[:1000]}
        
        ã“ã‚Œã‚‰ã®æƒ…å ±ã‹ã‚‰åˆ†é‡æ¨ªæ–­çš„ãªæ´å¯Ÿã‚’ç™ºè¦‹ã—ã€ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„:
        1. ç•°åˆ†é‡ã¨ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³
        2. æŠ€è¡“ã®æ–°ã—ã„çµ„ã¿åˆã‚ã›å¯èƒ½æ€§
        3. æœªæ¢ç´¢ã®ç ”ç©¶é ˜åŸŸ
        """
        
        response = await self._call_llm(system_prompt, user_prompt)
        
        return [{"insight": "Cross-domain insight example", "confidence": 0.8}]
    
    async def _propose_research_directions(self, analysis: Dict, 
                                         research_questions: List[str]) -> List[Dict]:
        """ç ”ç©¶æ–¹å‘ææ¡ˆ"""
        return [{"direction": "Novel research direction", "feasibility": 0.7, "impact": 0.9}]
    
    async def _generate_implementation_ideas(self, literature: Dict, analysis: Dict) -> List[Dict]:
        """å®Ÿè£…ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆ"""
        return [{"idea": "Implementation idea", "complexity": "medium", "novelty": 0.8}]
    
    async def _create_synthesis_summary(self, insights: List[Dict], 
                                      directions: List[Dict], ideas: List[Dict]) -> str:
        """çµ±åˆã‚µãƒãƒªãƒ¼ä½œæˆ"""
        return "Comprehensive synthesis summary..."
    
    async def _calculate_novelty_score(self, insights: List[Dict]) -> float:
        """æ–°è¦æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        return 0.8


class WriterAgent(BaseAgent):
    """è¨˜äº‹ä½œæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, agent_id: str, api_key: str):
        super().__init__(agent_id, AgentRole.WRITER, api_key)
    
    async def process_task(self, task: ResearchTask, 
                          synthesis_result: ResearchResult) -> ResearchResult:
        """è¨˜äº‹ä½œæˆå®Ÿè¡Œ"""
        logger.info(f"[{self.agent_id}] Starting article writing: {task.topic}")
        
        synthesis = synthesis_result.content
        
        # 1. è¨˜äº‹æ§‹æˆä½œæˆ
        article_structure = await self._create_article_structure(task, synthesis)
        
        # 2. å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŸ·ç­†
        sections = await self._write_article_sections(article_structure, synthesis)
        
        # 3. è¨˜äº‹çµ±åˆ
        full_article = await self._integrate_article(article_structure, sections)
        
        # 4. å“è³ªãƒã‚§ãƒƒã‚¯
        quality_score = await self._check_article_quality(full_article)
        
        # 5. SEOæœ€é©åŒ–
        seo_metadata = await self._optimize_seo(full_article, task.topic)
        
        article_result = {
            "article_structure": article_structure,
            "full_article": full_article,
            "quality_score": quality_score,
            "seo_metadata": seo_metadata,
            "word_count": len(full_article.split()),
            "estimated_reading_time": len(full_article.split()) // 200
        }
        
        result = ResearchResult(
            agent_id=self.agent_id,
            task_id=task.id,
            result_type="article",
            content=article_result,
            confidence_score=quality_score,
            created_at=datetime.now().isoformat()
        )
        
        self.save_result(result)
        
        # è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        await self._save_article_file(task, full_article, seo_metadata)
        
        return result
    
    async def _create_article_structure(self, task: ResearchTask, synthesis: Dict) -> Dict:
        """è¨˜äº‹æ§‹æˆä½œæˆ"""
        system_prompt = """
        æŠ€è¡“è¨˜äº‹ã®åŠ¹æœçš„ãªæ§‹æˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        èª­è€…ã«ã¨ã£ã¦ç†è§£ã—ã‚„ã™ãã€SEOã«ã‚‚æœ€é©åŒ–ã•ã‚ŒãŸæ§‹æˆã«ã—ã¦ãã ã•ã„ã€‚
        """
        
        user_prompt = f"""
        è¨˜äº‹ãƒˆãƒ”ãƒƒã‚¯: {task.topic}
        ç ”ç©¶çµæœã‚µãƒãƒªãƒ¼: {synthesis.get('synthesis_summary', '')[:500]}
        
        ä»¥ä¸‹ã‚’å«ã‚€è¨˜äº‹æ§‹æˆã‚’ä½œæˆã—ã¦ãã ã•ã„:
        1. ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆï¼ˆ3ã¤ï¼‰
        2. å°å…¥éƒ¨ã®è¦ç‚¹
        3. ä¸»è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦‹å‡ºã—
        4. å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¦‚è¦
        5. çµè«–éƒ¨ã®è¦ç‚¹
        """
        
        response = await self._call_llm(system_prompt, user_prompt)
        
        return {
            "titles": ["Sample Title 1", "Sample Title 2", "Sample Title 3"],
            "introduction": "Introduction points...",
            "sections": [
                {"title": "Background", "content": "Background content..."},
                {"title": "Analysis", "content": "Analysis content..."},
                {"title": "Implementation", "content": "Implementation content..."}
            ],
            "conclusion": "Conclusion points..."
        }
    
    async def _write_article_sections(self, structure: Dict, synthesis: Dict) -> Dict:
        """å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŸ·ç­†"""
        sections = {}
        
        for section in structure["sections"]:
            system_prompt = f"""
            æŠ€è¡“è¨˜äº‹ã®ã€Œ{section['title']}ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
            å°‚é–€çš„ã§ã‚ã‚ŠãªãŒã‚‰ç†è§£ã—ã‚„ã™ã„æ–‡ç« ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
            """
            
            user_prompt = f"""
            ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section['title']}
            æ¦‚è¦: {section['content']}
            å‚è€ƒæƒ…å ±: {json.dumps(synthesis, ensure_ascii=False)[:1000]}
            
            ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†…å®¹ã‚’è©³ã—ãåŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
            ã‚³ãƒ¼ãƒ‰ä¾‹ã‚„å›³è¡¨ã®ææ¡ˆã‚‚å«ã‚ã¦ãã ã•ã„ã€‚
            """
            
            content = await self._call_llm(system_prompt, user_prompt)
            sections[section['title']] = content
        
        return sections
    
    async def _integrate_article(self, structure: Dict, sections: Dict) -> str:
        """è¨˜äº‹çµ±åˆ"""
        article_parts = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        article_parts.append(f"# {structure['titles'][0]}")
        article_parts.append("")
        
        # å°å…¥
        article_parts.append("## ã¯ã˜ã‚ã«")
        article_parts.append(structure['introduction'])
        article_parts.append("")
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        for section in structure["sections"]:
            article_parts.append(f"## {section['title']}")
            article_parts.append(sections.get(section['title'], section['content']))
            article_parts.append("")
        
        # çµè«–
        article_parts.append("## ã¾ã¨ã‚")
        article_parts.append(structure['conclusion'])
        
        return "\n".join(article_parts)
    
    async def _check_article_quality(self, article: str) -> float:
        """è¨˜äº‹å“è³ªãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å“è³ªæŒ‡æ¨™
        word_count = len(article.split())
        structure_score = len([line for line in article.split('\n') if line.startswith('#')]) / 10
        content_density = word_count / 2000  # ç›®æ¨™2000èª
        
        return min((structure_score + content_density) / 2, 1.0)
    
    async def _optimize_seo(self, article: str, topic: str) -> Dict:
        """SEOæœ€é©åŒ–"""
        return {
            "meta_title": f"{topic} - æœ€æ–°AIæŠ€è¡“è§£èª¬",
            "meta_description": f"{topic}ã®æœ€æ–°å‹•å‘ã¨å®Ÿè£…æ–¹æ³•ã‚’è©³ã—ãè§£èª¬ã€‚AIç ”ç©¶ã®æœ€å‰ç·šã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚",
            "keywords": [topic, "AI", "machine learning", "implementation"],
            "og_title": f"{topic} - AIæŠ€è¡“è§£èª¬",
            "og_description": f"{topic}ã®è©³ç´°ãªæŠ€è¡“è§£èª¬ã¨å®Ÿè£…ã‚¬ã‚¤ãƒ‰"
        }
    
    async def _save_article_file(self, task: ResearchTask, article: str, seo: Dict):
        """è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
        articles_dir = Path("articles/generated")
        articles_dir.mkdir(parents=True, exist_ok=True)
        
        # Frontmatterä»˜ãMarkdownå½¢å¼ã§ä¿å­˜
        frontmatter = f"""---
title: "{seo['meta_title']}"
description: "{seo['meta_description']}"
keywords: {json.dumps(seo['keywords'])}
created_at: "{datetime.now().isoformat()}"
task_id: "{task.id}"
topic: "{task.topic}"
published: true
---

"""
        
        file_path = articles_dir / f"{task.id}_generated_article.md"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(frontmatter + article)
        
        logger.info(f"Article saved: {file_path}")


class CollaborativeResearchSystem:
    """å”èª¿ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.agents = {
            "researcher": ResearchAgent("researcher_001", api_key),
            "analyst": AnalysisAgent("analyst_001", api_key),
            "synthesizer": SynthesisAgent("synthesizer_001", api_key),
            "writer": WriterAgent("writer_001", api_key)
        }
        self.tasks: List[ResearchTask] = []
        self.results: Dict[str, List[ResearchResult]] = {}
    
    def create_research_task(self, topic: str, research_questions: List[str],
                           target_domains: List[str], priority: int = 1) -> str:
        """ç ”ç©¶ã‚¿ã‚¹ã‚¯ä½œæˆ"""
        task_id = f"TASK_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        task = ResearchTask(
            id=task_id,
            topic=topic,
            research_questions=research_questions,
            target_domains=target_domains,
            deadline=(datetime.now().strftime('%Y-%m-%d')),
            priority=priority
        )
        
        self.tasks.append(task)
        self.results[task_id] = []
        
        logger.info(f"Created research task: {task_id} - {topic}")
        return task_id
    
    async def execute_research_pipeline(self, task_id: str) -> Dict[str, ResearchResult]:
        """ç ”ç©¶ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        task = next((t for t in self.tasks if t.id == task_id), None)
        if not task:
            raise ValueError(f"Task not found: {task_id}")
        
        logger.info(f"Starting research pipeline: {task_id}")
        
        # 1. æ–‡çŒ®èª¿æŸ»
        literature_result = await self.agents["researcher"].process_task(task)
        self.results[task_id].append(literature_result)
        
        # 2. æŠ€è¡“åˆ†æ
        analysis_result = await self.agents["analyst"].process_task(task, literature_result)
        self.results[task_id].append(analysis_result)
        
        # 3. çŸ¥è­˜çµ±åˆ
        synthesis_result = await self.agents["synthesizer"].process_task(
            task, literature_result, analysis_result)
        self.results[task_id].append(synthesis_result)
        
        # 4. è¨˜äº‹ä½œæˆ
        article_result = await self.agents["writer"].process_task(task, synthesis_result)
        self.results[task_id].append(article_result)
        
        # ã‚¿ã‚¹ã‚¯å®Œäº†
        task.status = "completed"
        
        pipeline_results = {
            "literature": literature_result,
            "analysis": analysis_result,
            "synthesis": synthesis_result,
            "article": article_result
        }
        
        logger.info(f"Research pipeline completed: {task_id}")
        return pipeline_results
    
    def get_system_stats(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ"""
        total_tasks = len(self.tasks)
        completed_tasks = len([t for t in self.tasks if t.status == "completed"])
        total_results = sum(len(results) for results in self.results.values())
        
        return {
            "total_tasks": total_tasks,
            "completed_tasks": completed_tasks,
            "completion_rate": completed_tasks / total_tasks if total_tasks > 0 else 0,
            "total_results": total_results,
            "agents_count": len(self.agents)
        }


async def main():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    import os
    
    # API Keyè¨­å®š (ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("OPENAI_API_KEY not found in environment variables")
        return
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    research_system = CollaborativeResearchSystem(api_key)
    
    # ç ”ç©¶ã‚¿ã‚¹ã‚¯ä½œæˆ
    task_id = research_system.create_research_task(
        topic="Multi-Agent Reinforcement Learning",
        research_questions=[
            "What are the latest advances in multi-agent RL?",
            "How do agents coordinate in complex environments?",
            "What are the scalability challenges?"
        ],
        target_domains=["reinforcement-learning", "multi-agent-systems", "game-theory"],
        priority=1
    )
    
    # ç ”ç©¶ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    try:
        results = await research_system.execute_research_pipeline(task_id)
        
        print(f"\nğŸ‰ Research Pipeline Completed!")
        print(f"Task ID: {task_id}")
        print(f"Results Generated:")
        for result_type, result in results.items():
            print(f"  - {result_type}: {result.confidence_score:.2f} confidence")
        
        # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆè¡¨ç¤º
        stats = research_system.get_system_stats()
        print(f"\nğŸ“Š System Stats:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
            
    except Exception as e:
        logger.error(f"Research pipeline failed: {e}")


if __name__ == "__main__":
    asyncio.run(main())