#!/usr/bin/env python3
"""
AIå¼·åŒ–è¨˜äº‹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  - é«˜åº¦åŒ–ç‰ˆ
azukiazusa1ã‚¹ã‚¿ã‚¤ãƒ« Ã— ãƒ„ã‚§ãƒƒãƒ†ãƒ«ã‚«ã‚¹ãƒ†ãƒ³ Ã— AIå®Ÿé¨“çµ±åˆ

æ©Ÿèƒ½:
1. AIå®Ÿé¨“çµæœè‡ªå‹•è¨˜äº‹åŒ–
2. ãƒ„ã‚§ãƒƒãƒ†ãƒ«ã‚«ã‚¹ãƒ†ãƒ³çŸ¥è­˜çµ±åˆ
3. SEOæœ€é©åŒ–è¨˜äº‹ç”Ÿæˆ
4. å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
5. GitHubè‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
"""

import asyncio
import json
import logging
import os
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import yaml
import hashlib
import re
from dataclasses import dataclass, asdict

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import openai
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import sys
sys.path.append(str(Path(__file__).parent.parent))
from knowledge_graph.zettelkasten_ai_system import ZettelkastenAISystem, AIKnowledgeNote
from ai_experiments.multi_agent_systems.collaborative_research_agents import CollaborativeResearchSystem

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ArticleMetadata:
    """è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    id: str
    title: str
    description: str
    keywords: List[str]
    ai_domain: str
    experiment_id: Optional[str]
    knowledge_notes: List[str]
    quality_score: float
    seo_score: float
    estimated_reading_time: int
    created_at: str
    published: bool = False


@dataclass
class ContentAnalytics:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æçµæœ"""
    readability_score: float
    technical_depth: float
    novelty_score: float
    practical_value: float
    engagement_potential: float


class AIEnhancedArticleGenerator:
    """AIå¼·åŒ–è¨˜äº‹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: str = "config/article_generator.yaml"):
        self.config = self._load_config(config_path)
        self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.zettelkasten_system = ZettelkastenAISystem("knowledge-graph")
        self.research_system = CollaborativeResearchSystem(os.getenv("OPENAI_API_KEY"))
        
        # è¨˜äº‹ç”Ÿæˆè¨­å®š
        self.articles_path = Path("blog-app/articles")
        self.articles_path.mkdir(parents=True, exist_ok=True)
        
        # å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.article_templates = self._load_templates()
        
    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        default_config = {
            "generation": {
                "target_word_count": 2000,
                "min_quality_score": 0.7,
                "seo_optimization": True,
                "include_code_examples": True,
                "max_articles_per_day": 3
            },
            "content_strategy": {
                "primary_domains": ["llm", "agent", "rag", "prompt-engineering"],
                "content_types": ["tutorial", "analysis", "experiment-report", "concept-explanation"],
                "target_audience": ["researchers", "engineers", "students"],
                "tone": "technical-accessible"
            },
            "seo": {
                "target_keywords_per_article": 5,
                "meta_description_length": 160,
                "title_optimization": True,
                "internal_linking": True
            }
        }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
                default_config.update(user_config)
        except FileNotFoundError:
            logger.warning(f"Config file not found: {config_path}. Using defaults.")
        
        return default_config
    
    def _load_templates(self) -> Dict[str, str]:
        """è¨˜äº‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿"""
        return {
            "tutorial": """# {title}

## ã¯ã˜ã‚ã«
{introduction}

## èƒŒæ™¯ãƒ»åŸºç¤çŸ¥è­˜
{background}

## å®Ÿè£…æ–¹æ³•
{implementation}

## å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢
{examples}

## å¿œç”¨ãƒ»ç™ºå±•
{applications}

## ã¾ã¨ã‚
{conclusion}

## å‚è€ƒè³‡æ–™
{references}
""",
            "experiment-report": """# {title}

## å®Ÿé¨“æ¦‚è¦
{experiment_overview}

## ä»®èª¬ãƒ»ç›®çš„
{hypothesis}

## å®Ÿé¨“æ–¹æ³•
{methodology}

## çµæœãƒ»åˆ†æ
{results}

## è€ƒå¯Ÿãƒ»æ´å¯Ÿ
{discussion}

## ä»Šå¾Œã®å±•é–‹
{future_work}

## é–¢é€£çŸ¥è­˜ãƒ»å‚è€ƒæ–‡çŒ®
{related_knowledge}
""",
            "concept-explanation": """# {title}

## æ¦‚å¿µã®å®šç¾©
{definition}

## é‡è¦æ€§ãƒ»èƒŒæ™¯
{importance}

## æŠ€è¡“çš„è©³ç´°
{technical_details}

## å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
{implementation_patterns}

## ä½¿ç”¨ä¾‹ãƒ»ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£
{use_cases}

## ä»–ã®æ¦‚å¿µã¨ã®é–¢ä¿‚
{relationships}

## ã¾ã¨ã‚ãƒ»ä»Šå¾Œã®ç™ºå±•
{summary}
"""
        }
    
    async def generate_article_from_experiment(self, experiment_id: str) -> Optional[ArticleMetadata]:
        """å®Ÿé¨“çµæœã‹ã‚‰è¨˜äº‹ç”Ÿæˆ"""
        logger.info(f"Generating article from experiment: {experiment_id}")
        
        # 1. å®Ÿé¨“çµæœå–å¾—
        experiment_data = await self._get_experiment_data(experiment_id)
        if not experiment_data:
            logger.error(f"No experiment data found for: {experiment_id}")
            return None
        
        # 2. é–¢é€£çŸ¥è­˜ãƒãƒ¼ãƒˆå–å¾—
        related_notes = await self._get_related_knowledge_notes(experiment_data)
        
        # 3. è¨˜äº‹æ§‹æˆæ±ºå®š
        article_type = self._determine_article_type(experiment_data)
        template = self.article_templates[article_type]
        
        # 4. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
        content_sections = await self._generate_content_sections(
            experiment_data, related_notes, article_type)
        
        # 5. è¨˜äº‹çµ±åˆ
        full_article = self._integrate_article_content(template, content_sections)
        
        # 6. å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»æ”¹å–„
        quality_score = await self._assess_article_quality(full_article)
        if quality_score < self.config["generation"]["min_quality_score"]:
            full_article = await self._improve_article_quality(full_article, quality_score)
            quality_score = await self._assess_article_quality(full_article)
        
        # 7. SEOæœ€é©åŒ–
        seo_metadata = await self._optimize_for_seo(full_article, experiment_data)
        
        # 8. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        article_metadata = await self._create_article_metadata(
            experiment_data, related_notes, quality_score, seo_metadata)
        
        # 9. ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        await self._save_article_with_metadata(full_article, article_metadata)
        
        # 10. çŸ¥è­˜ã‚°ãƒ©ãƒ•æ›´æ–°
        await self._update_knowledge_graph_with_article(article_metadata, related_notes)
        
        logger.info(f"Article generated successfully: {article_metadata.id}")
        return article_metadata
    
    async def _get_experiment_data(self, experiment_id: str) -> Optional[Dict]:
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        results_path = Path(f"results/synthesizer_001/{experiment_id}_synthesis.json")
        
        if results_path.exists():
            with open(results_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # å®Ÿé¨“ãŒå®Œäº†ã—ã¦ã„ãªã„å ´åˆã¯å¾…æ©Ÿ
        logger.info(f"Experiment data not found, checking research system...")
        
        # ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦è¡Œ
        try:
            stats = self.research_system.get_system_stats()
            if stats["completed_tasks"] > 0:
                # æœ€æ–°å®Œäº†ã‚¿ã‚¹ã‚¯ã®çµæœã‚’å–å¾— (å®Ÿè£…ç°¡ç•¥åŒ–)
                return {
                    "experiment_id": experiment_id,
                    "topic": "Multi-Agent AI Systems",
                    "results": {"summary": "Experimental AI research results..."},
                    "insights": ["Insight 1", "Insight 2"],
                    "implementation": {"code": "# Sample implementation code"},
                    "evaluation": {"accuracy": 0.85, "performance": "good"}
                }
        except Exception as e:
            logger.error(f"Error getting experiment data: {e}")
        
        return None
    
    async def _get_related_knowledge_notes(self, experiment_data: Dict) -> List[AIKnowledgeNote]:
        """é–¢é€£çŸ¥è­˜ãƒãƒ¼ãƒˆå–å¾—"""
        topic = experiment_data.get("topic", "")
        
        # ãƒ„ã‚§ãƒƒãƒ†ãƒ«ã‚«ã‚¹ãƒ†ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰é–¢é€£ãƒãƒ¼ãƒˆæ¤œç´¢
        stats = self.zettelkasten_system.get_knowledge_stats()
        
        if stats["total_notes"] > 0:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨
            return [
                AIKnowledgeNote(
                    id="NOTE_001",
                    title="Multi-Agent Coordination",
                    content="Multi-agent systems coordination mechanisms...",
                    ai_domain="agent",
                    experiment_id=experiment_data.get("experiment_id"),
                    concepts=["coordination", "multi-agent", "consensus"],
                    connections=["NOTE_002", "NOTE_003"],
                    created_at=datetime.now().isoformat(),
                    updated_at=datetime.now().isoformat(),
                    permanence_score=0.8,
                    emergence_potential=0.7
                )
            ]
        
        return []
    
    def _determine_article_type(self, experiment_data: Dict) -> str:
        """è¨˜äº‹ã‚¿ã‚¤ãƒ—æ±ºå®š"""
        topic = experiment_data.get("topic", "").lower()
        
        if "implementation" in experiment_data or "code" in str(experiment_data):
            return "tutorial"
        elif "experiment" in topic or "evaluation" in experiment_data:
            return "experiment-report"
        else:
            return "concept-explanation"
    
    async def _generate_content_sections(self, experiment_data: Dict, 
                                       related_notes: List[AIKnowledgeNote], 
                                       article_type: str) -> Dict[str, str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        sections = {}
        
        if article_type == "experiment-report":
            sections = await self._generate_experiment_report_sections(experiment_data, related_notes)
        elif article_type == "tutorial":
            sections = await self._generate_tutorial_sections(experiment_data, related_notes)
        else:  # concept-explanation
            sections = await self._generate_concept_sections(experiment_data, related_notes)
        
        return sections
    
    async def _generate_experiment_report_sections(self, experiment_data: Dict,
                                                 related_notes: List[AIKnowledgeNote]) -> Dict[str, str]:
        """å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        sections = {}
        
        # å®Ÿé¨“æ¦‚è¦
        sections["experiment_overview"] = await self._generate_section_content(
            "å®Ÿé¨“æ¦‚è¦",
            f"å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿: {json.dumps(experiment_data, ensure_ascii=False)[:500]}",
            "ã“ã®å®Ÿé¨“ã®ç›®çš„ã€å¯¾è±¡æŠ€è¡“ã€æœŸå¾…ã•ã‚Œã‚‹æˆæœã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        )
        
        # ä»®èª¬ãƒ»ç›®çš„
        sections["hypothesis"] = await self._generate_section_content(
            "ä»®èª¬ãƒ»ç›®çš„",
            f"ãƒˆãƒ”ãƒƒã‚¯: {experiment_data.get('topic', '')}",
            "ã“ã®å®Ÿé¨“ã§æ¤œè¨¼ã—ãŸã„ä»®èª¬ã¨å…·ä½“çš„ãªç›®çš„ã‚’æ˜ç¢ºã«è¿°ã¹ã¦ãã ã•ã„ã€‚"
        )
        
        # å®Ÿé¨“æ–¹æ³•
        sections["methodology"] = await self._generate_section_content(
            "å®Ÿé¨“æ–¹æ³•",
            f"å®Ÿè£…è©³ç´°: {experiment_data.get('implementation', {})}",
            "å®Ÿé¨“ã®æ‰‹é †ã€ä½¿ç”¨ã—ãŸãƒ„ãƒ¼ãƒ«ã€è©•ä¾¡æŒ‡æ¨™ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ¼ãƒ‰ä¾‹ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚"
        )
        
        # çµæœãƒ»åˆ†æ
        sections["results"] = await self._generate_section_content(
            "çµæœãƒ»åˆ†æ",
            f"è©•ä¾¡çµæœ: {experiment_data.get('evaluation', {})}",
            "å®Ÿé¨“çµæœã‚’å®šé‡çš„ãƒ»å®šæ€§çš„ã«åˆ†æã—ã€ã‚°ãƒ©ãƒ•ã‚„è¡¨ã‚’ç”¨ã„ã¦è¦–è¦šåŒ–ã—ã¦ãã ã•ã„ã€‚"
        )
        
        # è€ƒå¯Ÿãƒ»æ´å¯Ÿ
        related_context = "\n".join([f"é–¢é€£çŸ¥è­˜: {note.title} - {note.content[:200]}" 
                                   for note in related_notes])
        sections["discussion"] = await self._generate_section_content(
            "è€ƒå¯Ÿãƒ»æ´å¯Ÿ",
            f"å®Ÿé¨“æ´å¯Ÿ: {experiment_data.get('insights', [])}\n{related_context}",
            "çµæœã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ´å¯Ÿã€æ—¢å­˜ç ”ç©¶ã¨ã®æ¯”è¼ƒã€ç™ºè¦‹ã•ã‚ŒãŸæ–°ã—ã„çŸ¥è¦‹ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚"
        )
        
        # ä»Šå¾Œã®å±•é–‹
        sections["future_work"] = await self._generate_section_content(
            "ä»Šå¾Œã®å±•é–‹",
            f"å®Ÿé¨“çµæœ: {experiment_data.get('results', {})}",
            "ã“ã®å®Ÿé¨“çµæœã‚’å—ã‘ã¦ã€ä»Šå¾Œã®ç ”ç©¶æ–¹å‘ã‚„æ”¹å–„ç‚¹ã«ã¤ã„ã¦ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        
        # é–¢é€£çŸ¥è­˜ãƒ»å‚è€ƒæ–‡çŒ®
        sections["related_knowledge"] = self._generate_related_knowledge_section(related_notes)
        
        return sections
    
    async def _generate_tutorial_sections(self, experiment_data: Dict,
                                        related_notes: List[AIKnowledgeNote]) -> Dict[str, str]:
        """ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        sections = {}
        
        sections["introduction"] = await self._generate_section_content(
            "å°å…¥",
            f"ãƒˆãƒ”ãƒƒã‚¯: {experiment_data.get('topic', '')}",
            "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§å­¦ç¿’ã™ã‚‹å†…å®¹ã€å‰æçŸ¥è­˜ã€æœŸå¾…ã•ã‚Œã‚‹å­¦ç¿’æˆæœã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["background"] = await self._generate_section_content(
            "èƒŒæ™¯ãƒ»åŸºç¤çŸ¥è­˜",
            "\n".join([note.content[:300] for note in related_notes]),
            "å¿…è¦ãªèƒŒæ™¯çŸ¥è­˜ã€åŸºç¤æ¦‚å¿µã‚’åˆå­¦è€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["implementation"] = await self._generate_section_content(
            "å®Ÿè£…æ–¹æ³•",
            f"å®Ÿè£…: {experiment_data.get('implementation', {})}",
            "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè£…æ‰‹é †ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€æ³¨æ„äº‹é …ã‚’å«ã‚ã¦ãã ã•ã„ã€‚å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ä¾‹ã‚‚æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["examples"] = await self._generate_section_content(
            "å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢",
            f"è©•ä¾¡çµæœ: {experiment_data.get('evaluation', {})}",
            "å®Ÿéš›ã®ä½¿ç”¨ä¾‹ã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›çµæœã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["applications"] = await self._generate_section_content(
            "å¿œç”¨ãƒ»ç™ºå±•",
            f"æ´å¯Ÿ: {experiment_data.get('insights', [])}",
            "åŸºæœ¬å®Ÿè£…ã‹ã‚‰ã®ç™ºå±•æ–¹æ³•ã€å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®å¿œç”¨ä¾‹ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["conclusion"] = await self._generate_section_content(
            "ã¾ã¨ã‚",
            f"å®Ÿé¨“çµæœå…¨ä½“: {json.dumps(experiment_data, ensure_ascii=False)[:300]}",
            "å­¦ç¿’ã—ãŸå†…å®¹ã®ã¾ã¨ã‚ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã®å†ç¢ºèªã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["references"] = self._generate_references_section(experiment_data, related_notes)
        
        return sections
    
    async def _generate_concept_sections(self, experiment_data: Dict,
                                       related_notes: List[AIKnowledgeNote]) -> Dict[str, str]:
        """æ¦‚å¿µè§£èª¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        sections = {}
        
        sections["definition"] = await self._generate_section_content(
            "æ¦‚å¿µã®å®šç¾©",
            f"ãƒˆãƒ”ãƒƒã‚¯: {experiment_data.get('topic', '')}",
            "ã“ã®æ¦‚å¿µã®æ­£ç¢ºãªå®šç¾©ã€é¡ä¼¼æ¦‚å¿µã¨ã®é•ã„ã€æŠ€è¡“çš„ç‰¹å¾´ã‚’æ˜ç¢ºã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["importance"] = await self._generate_section_content(
            "é‡è¦æ€§ãƒ»èƒŒæ™¯",
            f"å®Ÿé¨“èƒŒæ™¯: {experiment_data.get('results', {})}",
            "ãªãœã“ã®æ¦‚å¿µãŒé‡è¦ãªã®ã‹ã€ç¾åœ¨ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["technical_details"] = await self._generate_section_content(
            "æŠ€è¡“çš„è©³ç´°",
            f"å®Ÿè£…è©³ç´°: {experiment_data.get('implementation', {})}",
            "æŠ€è¡“çš„ãªä»•çµ„ã¿ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€æ•°å­¦çš„åŸºç¤ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["implementation_patterns"] = await self._generate_section_content(
            "å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³",
            "\n".join([note.content[:200] for note in related_notes]),
            "ä¸€èˆ¬çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€é¿ã‘ã‚‹ã¹ãã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["use_cases"] = await self._generate_section_content(
            "ä½¿ç”¨ä¾‹ãƒ»ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£",
            f"è©•ä¾¡äº‹ä¾‹: {experiment_data.get('evaluation', {})}",
            "å®Ÿéš›ã®ä½¿ç”¨ä¾‹ã€æˆåŠŸäº‹ä¾‹ã€å¤±æ•—äº‹ä¾‹ã‹ã‚‰å­¦ã¹ã‚‹æ•™è¨“ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["relationships"] = await self._generate_section_content(
            "ä»–ã®æ¦‚å¿µã¨ã®é–¢ä¿‚",
            "\n".join([f"{note.title}: {', '.join(note.concepts)}" for note in related_notes]),
            "é–¢é€£ã™ã‚‹æ¦‚å¿µã€ä¸Šä½ãƒ»ä¸‹ä½æ¦‚å¿µã¨ã®é–¢ä¿‚ã€æŠ€è¡“çš„ãªä¾å­˜é–¢ä¿‚ã‚’å›³è§£ã—ã¦ãã ã•ã„ã€‚"
        )
        
        sections["summary"] = await self._generate_section_content(
            "ã¾ã¨ã‚ãƒ»ä»Šå¾Œã®ç™ºå±•",
            f"ä»Šå¾Œã®æ´å¯Ÿ: {experiment_data.get('insights', [])}",
            "æ¦‚å¿µã®é‡è¦ãƒã‚¤ãƒ³ãƒˆæ•´ç†ã€ä»Šå¾Œã®ç™ºå±•æ–¹å‘ã€ç ”ç©¶ãƒ»é–‹ç™ºã®å±•æœ›ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚"
        )
        
        return sections
    
    async def _generate_section_content(self, section_title: str, context: str, 
                                      instruction: str) -> str:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        system_prompt = f"""
        ã‚ãªãŸã¯æŠ€è¡“è¨˜äº‹åŸ·ç­†ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
        azukiazusa1ã®ã‚ˆã†ãªé«˜å“è³ªã§å®Ÿç”¨çš„ãªæŠ€è¡“è¨˜äº‹ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
        
        è¨˜äº‹ã®ç‰¹å¾´:
        - å®Ÿè£…ä¾‹ã¨ã‚³ãƒ¼ãƒ‰ã‚’é‡è¦–
        - åˆå­¦è€…ã«ã‚‚ç†è§£ã—ã‚„ã™ã„èª¬æ˜
        - æœ€æ–°æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åæ˜ 
        - SEOã‚’æ„è­˜ã—ãŸæ§‹æˆ
        """
        
        user_prompt = f"""
        ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section_title}
        
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
        {context}
        
        æŒ‡ç¤º:
        {instruction}
        
        ã“ã®æŠ€è¡“è¨˜äº‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã€ä»¥ä¸‹ã®ç‚¹ã‚’è€ƒæ…®ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„:
        1. å…·ä½“ä¾‹ã¨ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’å«ã‚ã‚‹
        2. å›³è¡¨ã‚„ç®‡æ¡æ›¸ãã§ç†è§£ã—ã‚„ã™ãã™ã‚‹
        3. 2-3æ®µè½ã€500-800èªç¨‹åº¦
        4. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›
        5. å®Ÿç”¨æ€§ã‚’é‡è¦–ã—ãŸå†…å®¹
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error generating section content: {e}")
            return f"## {section_title}\n\n[Content generation error]"
    
    def _generate_related_knowledge_section(self, related_notes: List[AIKnowledgeNote]) -> str:
        """é–¢é€£çŸ¥è­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        if not related_notes:
            return "## å‚è€ƒè³‡æ–™\n\n- [AIæŠ€è¡“ã®æœ€æ–°å‹•å‘](https://example.com/ai-trends)\n- [å®Ÿè£…ã‚¬ã‚¤ãƒ‰](https://example.com/implementation-guide)"
        
        content = ["## é–¢é€£çŸ¥è­˜ãƒ»å‚è€ƒæ–‡çŒ®\n"]
        
        for note in related_notes:
            content.append(f"### {note.title}")
            content.append(f"{note.content[:200]}...")
            content.append(f"**é–¢é€£æ¦‚å¿µ**: {', '.join(note.concepts)}")
            content.append("")
        
        return "\n".join(content)
    
    def _generate_references_section(self, experiment_data: Dict, 
                                   related_notes: List[AIKnowledgeNote]) -> str:
        """å‚è€ƒæ–‡çŒ®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        references = ["## å‚è€ƒè³‡æ–™\n"]
        
        # å®Ÿé¨“é–¢é€£ã®å‚è€ƒæ–‡çŒ®
        if "sources" in experiment_data:
            references.extend([f"- {source}" for source in experiment_data["sources"]])
        
        # çŸ¥è­˜ãƒãƒ¼ãƒˆã‹ã‚‰ã®å‚è€ƒæ–‡çŒ®
        for note in related_notes:
            references.append(f"- [{note.title}](knowledge-graph/{note.id}.md)")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚è€ƒæ–‡çŒ®
        references.extend([
            "- [OpenAI API Documentation](https://platform.openai.com/docs)",
            "- [Multi-Agent Systems Research](https://arxiv.org/list/cs.MA/recent)",
            "- [AIå®Ÿè£…ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](https://github.com/topics/artificial-intelligence)"
        ])
        
        return "\n".join(references)
    
    def _integrate_article_content(self, template: str, sections: Dict[str, str]) -> str:
        """è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ±åˆ"""
        try:
            return template.format(**sections)
        except KeyError as e:
            logger.error(f"Template formatting error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é †ç•ªã«çµåˆ
            content_parts = []
            for section_name, section_content in sections.items():
                content_parts.append(section_content)
                content_parts.append("")
            return "\n".join(content_parts)
    
    async def _assess_article_quality(self, article: str) -> float:
        """è¨˜äº‹å“è³ªè©•ä¾¡"""
        analytics = await self._analyze_content(article)
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®— (0-1)
        quality_factors = {
            "length": min(len(article.split()) / 2000, 1.0),  # ç›®æ¨™2000èª
            "structure": len(re.findall(r'^##?\s', article, re.MULTILINE)) / 8,  # ç›®æ¨™8è¦‹å‡ºã—
            "code_examples": len(re.findall(r'```', article)) / 4,  # ç›®æ¨™2ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
            "readability": analytics.readability_score,
            "technical_depth": analytics.technical_depth,
            "practical_value": analytics.practical_value
        }
        
        weighted_score = (
            quality_factors["length"] * 0.2 +
            quality_factors["structure"] * 0.15 +
            quality_factors["code_examples"] * 0.2 +
            quality_factors["readability"] * 0.15 +
            quality_factors["technical_depth"] * 0.15 +
            quality_factors["practical_value"] * 0.15
        )
        
        return min(weighted_score, 1.0)
    
    async def _analyze_content(self, article: str) -> ContentAnalytics:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ"""
        # ç°¡æ˜“åˆ†æå®Ÿè£…
        word_count = len(article.split())
        sentence_count = len(re.findall(r'[.!?]+', article))
        
        # èª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢ (Flesch Reading Easeç°¡æ˜“ç‰ˆ)
        avg_sentence_length = word_count / max(sentence_count, 1)
        readability_score = max(0, min(1, (20 - avg_sentence_length) / 20))
        
        # æŠ€è¡“çš„æ·±åº¦ (æŠ€è¡“ç”¨èªã®å¯†åº¦)
        tech_terms = len(re.findall(r'\b(API|algorithm|implementation|framework|system|model|data|function|class|method)\b', article, re.IGNORECASE))
        technical_depth = min(tech_terms / 20, 1.0)
        
        # å®Ÿç”¨ä¾¡å€¤ (ã‚³ãƒ¼ãƒ‰ä¾‹ã€å®Ÿä¾‹ã®æœ‰ç„¡)
        code_blocks = len(re.findall(r'```', article))
        examples = len(re.findall(r'(ä¾‹|example|å®Ÿè£…|implementation)', article, re.IGNORECASE))
        practical_value = min((code_blocks + examples) / 5, 1.0)
        
        # æ–°è¦æ€§ã‚¹ã‚³ã‚¢ (æ–°ã—ã„æ¦‚å¿µã‚„æ‰‹æ³•ã®è¨€åŠ)
        novelty_indicators = len(re.findall(r'(æ–°ã—ã„|æœ€æ–°|é©æ–°|breakthrough|novel|cutting-edge)', article, re.IGNORECASE))
        novelty_score = min(novelty_indicators / 3, 1.0)
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå¯èƒ½æ€§ (å•ã„ã‹ã‘ã€å‘¼ã³ã‹ã‘ã®æœ‰ç„¡)
        engagement_indicators = len(re.findall(r'(\?|ã©ã†|ã„ã‹ãŒ|è©¦ã—ã¦|æŒ‘æˆ¦|è€ƒãˆã¦)', article, re.IGNORECASE))
        engagement_potential = min(engagement_indicators / 5, 1.0)
        
        return ContentAnalytics(
            readability_score=readability_score,
            technical_depth=technical_depth,
            novelty_score=novelty_score,
            practical_value=practical_value,
            engagement_potential=engagement_potential
        )
    
    async def _improve_article_quality(self, article: str, current_score: float) -> str:
        """è¨˜äº‹å“è³ªæ”¹å–„"""
        improvement_prompt = f"""
        ä»¥ä¸‹ã®æŠ€è¡“è¨˜äº‹ã®å“è³ªã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„ã€‚
        ç¾åœ¨ã®å“è³ªã‚¹ã‚³ã‚¢: {current_score:.2f}
        
        æ”¹å–„ç‚¹:
        1. ã‚ˆã‚Šå…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã‚’è¿½åŠ 
        2. å®Ÿç”¨çš„ãªå®Ÿè£…æ‰‹é †ã‚’è©³ç´°åŒ–
        3. èª­ã¿ã‚„ã™ã•ã®å‘ä¸Š
        4. æŠ€è¡“çš„æ·±åº¦ã®å‘ä¸Š
        
        è¨˜äº‹:
        {article}
        
        æ”¹å–„ã•ã‚ŒãŸè¨˜äº‹ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": improvement_prompt}],
                temperature=0.5,
                max_tokens=3000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error improving article quality: {e}")
            return article
    
    async def _optimize_for_seo(self, article: str, experiment_data: Dict) -> Dict:
        """SEOæœ€é©åŒ–"""
        topic = experiment_data.get("topic", "AIæŠ€è¡“")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = await self._extract_seo_keywords(article, topic)
        
        # ãƒ¡ã‚¿æƒ…å ±ç”Ÿæˆ
        meta_title = f"{topic} - å®Ÿè£…ã‚¬ã‚¤ãƒ‰ã¨å®Ÿé¨“çµæœ"
        meta_description = f"{topic}ã®è©³ç´°ãªå®Ÿè£…æ–¹æ³•ã¨å®Ÿé¨“çµæœã‚’è§£èª¬ã€‚æœ€æ–°AIæŠ€è¡“ã®å®Ÿç”¨çš„æ´»ç”¨æ³•ã‚’ã‚³ãƒ¼ãƒ‰ä¾‹ä»˜ãã§ç´¹ä»‹ã—ã¾ã™ã€‚"
        
        return {
            "meta_title": meta_title,
            "meta_description": meta_description[:160],  # 160æ–‡å­—åˆ¶é™
            "keywords": keywords,
            "og_title": meta_title,
            "og_description": meta_description[:200],
            "twitter_card": "summary_large_image",
            "canonical_url": f"https://blog-app-kappa-sand.vercel.app/blog/{self._generate_slug(topic)}"
        }
    
    async def _extract_seo_keywords(self, article: str, main_topic: str) -> List[str]:
        """SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        keywords = [main_topic.lower()]
        
        # æŠ€è¡“ç”¨èªæŠ½å‡º
        tech_patterns = [
            r'\b(AI|machine\s+learning|deep\s+learning|neural\s+network)\b',
            r'\b(API|framework|library|implementation)\b',
            r'\b(Python|JavaScript|TypeScript|React|Node\.js)\b',
            r'\b(algorithm|model|data|training|evaluation)\b'
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, article, re.IGNORECASE)
            keywords.extend([match.lower() for match in matches if isinstance(match, str)])
        
        # é‡è¤‡é™¤å»ãƒ»é »åº¦é †ã‚½ãƒ¼ãƒˆ
        keyword_freq = {}
        for keyword in keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
        
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        return [kw for kw, freq in sorted_keywords[:self.config["seo"]["target_keywords_per_article"]]]
    
    def _generate_slug(self, title: str) -> str:
        """URL slugç”Ÿæˆ"""
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[-\s]+', '-', slug)
        return slug.strip('-')
    
    async def _create_article_metadata(self, experiment_data: Dict, 
                                     related_notes: List[AIKnowledgeNote],
                                     quality_score: float, seo_metadata: Dict) -> ArticleMetadata:
        """è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        article_id = f"AI_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return ArticleMetadata(
            id=article_id,
            title=seo_metadata["meta_title"],
            description=seo_metadata["meta_description"],
            keywords=seo_metadata["keywords"],
            ai_domain=experiment_data.get("domain", "general"),
            experiment_id=experiment_data.get("experiment_id"),
            knowledge_notes=[note.id for note in related_notes],
            quality_score=quality_score,
            seo_score=await self._calculate_seo_score(seo_metadata),
            estimated_reading_time=len(seo_metadata["meta_description"].split()) // 200 * 60,  # æ¦‚ç®—
            created_at=datetime.now().isoformat(),
            published=quality_score >= self.config["generation"]["min_quality_score"]
        )
    
    async def _calculate_seo_score(self, seo_metadata: Dict) -> float:
        """SEOã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score_factors = {
            "title_length": 1.0 if 30 <= len(seo_metadata["meta_title"]) <= 60 else 0.5,
            "description_length": 1.0 if 120 <= len(seo_metadata["meta_description"]) <= 160 else 0.5,
            "keywords_count": min(len(seo_metadata["keywords"]) / 5, 1.0),
            "has_og_tags": 1.0 if "og_title" in seo_metadata else 0.0
        }
        
        return sum(score_factors.values()) / len(score_factors)
    
    async def _save_article_with_metadata(self, article: str, metadata: ArticleMetadata):
        """è¨˜äº‹ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        # Frontmatterä½œæˆ
        frontmatter = f"""---
id: "{metadata.id}"
title: "{metadata.title}"
description: "{metadata.description}"
keywords: {json.dumps(metadata.keywords)}
ai_domain: "{metadata.ai_domain}"
experiment_id: "{metadata.experiment_id}"
knowledge_notes: {json.dumps(metadata.knowledge_notes)}
quality_score: {metadata.quality_score:.3f}
seo_score: {metadata.seo_score:.3f}
estimated_reading_time: {metadata.estimated_reading_time}
created_at: "{metadata.created_at}"
published: {str(metadata.published).lower()}
emoji: "ğŸ§ "
type: "tech"
topics: {json.dumps(metadata.keywords[:3])}
published_at: "{datetime.now().strftime('%Y-%m-%d')}"
---

"""
        
        # è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        article_file = self.articles_path / f"{metadata.id}.md"
        with open(article_file, 'w', encoding='utf-8') as f:
            f.write(frontmatter + article)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        metadata_file = self.articles_path / f"{metadata.id}_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(metadata), f, ensure_ascii=False, indent=2)
        
        logger.info(f"Article saved: {article_file}")
    
    async def _update_knowledge_graph_with_article(self, metadata: ArticleMetadata, 
                                                 related_notes: List[AIKnowledgeNote]):
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•æ›´æ–°"""
        # è¨˜äº‹ã‚’æ–°ã—ã„çŸ¥è­˜ãƒãƒ¼ãƒˆã¨ã—ã¦è¿½åŠ 
        article_note_id = self.zettelkasten_system.create_note(
            title=metadata.title,
            content=f"Generated article about {metadata.ai_domain}",
            ai_domain=metadata.ai_domain,
            experiment_id=metadata.experiment_id
        )
        
        logger.info(f"Knowledge graph updated with article: {article_note_id}")
    
    async def auto_generate_daily_articles(self, max_articles: int = None) -> List[ArticleMetadata]:
        """æ—¥æ¬¡è‡ªå‹•è¨˜äº‹ç”Ÿæˆ"""
        if max_articles is None:
            max_articles = self.config["generation"]["max_articles_per_day"]
        
        logger.info(f"Starting daily article generation (max: {max_articles})")
        
        generated_articles = []
        
        # 1. æ–°ã—ã„å®Ÿé¨“çµæœã‚’ãƒã‚§ãƒƒã‚¯
        experiment_ids = await self._get_new_experiment_results()
        
        # 2. å„å®Ÿé¨“ã‹ã‚‰è¨˜äº‹ç”Ÿæˆ
        for i, experiment_id in enumerate(experiment_ids[:max_articles]):
            try:
                article_metadata = await self.generate_article_from_experiment(experiment_id)
                if article_metadata:
                    generated_articles.append(article_metadata)
                    logger.info(f"Generated article {i+1}/{min(len(experiment_ids), max_articles)}")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
                    await asyncio.sleep(2)
                    
            except Exception as e:
                logger.error(f"Error generating article for {experiment_id}: {e}")
        
        # 3. GitHubè‡ªå‹•ã‚³ãƒŸãƒƒãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        if generated_articles and os.getenv("AUTO_COMMIT", "false").lower() == "true":
            await self._auto_commit_articles(generated_articles)
        
        logger.info(f"Daily article generation completed: {len(generated_articles)} articles")
        return generated_articles
    
    async def _get_new_experiment_results(self) -> List[str]:
        """æ–°ã—ã„å®Ÿé¨“çµæœå–å¾—"""
        # éå»24æ™‚é–“ã®å®Ÿé¨“çµæœã‚’æ¤œç´¢
        results_dir = Path("results")
        if not results_dir.exists():
            return ["DEMO_EXPERIMENT_001"]  # ãƒ‡ãƒ¢ç”¨
        
        cutoff_time = datetime.now() - timedelta(days=1)
        new_experiments = []
        
        for result_file in results_dir.rglob("*_synthesis.json"):
            if result_file.stat().st_mtime > cutoff_time.timestamp():
                experiment_id = result_file.stem.replace("_synthesis", "")
                new_experiments.append(experiment_id)
        
        return new_experiments[:5]  # æœ€å¤§5å®Ÿé¨“
    
    async def _auto_commit_articles(self, generated_articles: List[ArticleMetadata]):
        """è‡ªå‹•Git ã‚³ãƒŸãƒƒãƒˆ"""
        try:
            # Git add
            subprocess.run(["git", "add", "blog-app/articles/"], check=True, cwd=".")
            
            # Commit messageä½œæˆ
            article_titles = [metadata.title[:50] for metadata in generated_articles]
            commit_message = f"ğŸ¤– Auto-generated {len(generated_articles)} AI articles\n\n" + \
                           "\n".join([f"- {title}" for title in article_titles])
            
            # Git commit
            subprocess.run(["git", "commit", "-m", commit_message], check=True, cwd=".")
            
            # Git push (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            if os.getenv("AUTO_PUSH", "false").lower() == "true":
                subprocess.run(["git", "push"], check=True, cwd=".")
            
            logger.info(f"Articles committed to Git: {len(generated_articles)} articles")
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Git operations failed: {e}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    generator = AIEnhancedArticleGenerator()
    
    # ãƒ‡ãƒ¢: å®Ÿé¨“çµæœã‹ã‚‰è¨˜äº‹ç”Ÿæˆ
    article_metadata = await generator.generate_article_from_experiment("DEMO_EXPERIMENT_001")
    
    if article_metadata:
        print(f"âœ… Article generated successfully!")
        print(f"ID: {article_metadata.id}")
        print(f"Title: {article_metadata.title}")
        print(f"Quality Score: {article_metadata.quality_score:.3f}")
        print(f"SEO Score: {article_metadata.seo_score:.3f}")
        print(f"Published: {article_metadata.published}")
    else:
        print("âŒ Failed to generate article")


if __name__ == "__main__":
    asyncio.run(main())